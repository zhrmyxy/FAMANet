We thank the reviewer for the comments.

### Response to Reviewer#1
# Response to PAAM module
æˆ‘ä»¬ç¡®è®¤Waå’ŒWpæ˜¯é€šè¿‡ç«¯åˆ°ç«¯çš„åå‘ä¼ æ’­è‡ªé€‚åº”å­¦ä¹ å¾—åˆ°çš„ï¼Œè®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§çš„å‚æ•°ç”Ÿæˆç½‘ç»œï¼Œè¯¥å­ç½‘ç»œåŒ…å«å…¨å±€å¹³å‡æ± åŒ–å±‚å’Œä¸¤å±‚MLPï¼ˆLinear -> ReLU -> Linear -> Softmaxï¼‰ã€‚
å¯¹äºå…±äº«å˜æ¢ï¼Œæˆ‘ä»¬ä½¿ç”¨çº¿æ€§å±‚è¿›è¡Œé™ç»´çš„æ—¶å€™ä½¿ç”¨äº†å…±äº«çš„çº¿æ€§å±‚ï¼Œè€Œç”Ÿæˆå„è‡ªå˜æ¢æ—¶ä½¿ç”¨ç‹¬ç«‹çš„çº¿æ€§å±‚ï¼Œå…±äº«æƒé‡é€šè¿‡å‡å°‘å‚æ•°é‡èµ·åˆ°äº†æ­£åˆ™åŒ–ä½œç”¨ï¼Œé˜²æ­¢äº†å°æ ·æœ¬ä»»åŠ¡ä¸­çš„è¿‡æ‹Ÿåˆï¼Œå¼ºåˆ¶å¹…åº¦å’Œç›¸ä½ç‰¹å¾åœ¨ç»Ÿä¸€çš„æ½œç©ºé—´ä¸­å¯¹é½ã€‚
å¯¹äºæƒé‡æ¶ˆèç ”ç©¶å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š
<div align="center">
  <img width="676" height="139" alt="image" src="https://github.com/user-attachments/assets/c8a1b3d4-d058-4146-adf5-26b395a69606" />
</div>

### Cost Analysis of AMAM Calculation
AMAMæ¨¡å—æ˜¯æ— å‚æ•°çš„ï¼Œå…¶è¿ç®—ä¸»è¦æ˜¯ç”±çŸ©é˜µä¹˜æ³•å¾—åˆ°ï¼Œè®¡ç®—å¤æ‚åº¦æ˜¯Oï¼ˆN2ï¼‰
<div align="center">
  <img width="621" height="90" alt="image" src="https://github.com/user-attachments/assets/6d0acfc7-adcb-4f26-ae37-f4d5e9acf01f" />
</div>
ç”±å®éªŒç»“æœå¯è§ï¼ŒAMAMçš„è®¡ç®—æˆæœ¬æä½ï¼Œå¯¹äºæ¨¡å‹æ¨ç†é€Ÿåº¦å‡ ä¹ä¸å½±å“ã€‚

å«å™ªå£°æ”¯æŒæ©ç å®éªŒï¼šä¸ºäº†è¯„ä¼°æ¨¡å‹å¯¹ä¸å®Œç¾æ ‡æ³¨çš„é²æ£’æ€§ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹ æ”¯æŒ æ©ç åº”ç”¨å½¢æ€å­¦è†¨èƒ€æ¥æ¨¡æ‹Ÿå™ªå£°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸åŒå¤§å°çš„æ ¸æ¥æ‰©å±•æ©ç è¾¹ç•Œã€‚è¿™ä¸€è¿‡ç¨‹ä¸å¯é¿å…åœ°å°†èƒŒæ™¯æ‚è´¨å¼•å…¥åˆ°æ”¯æŒç‰¹å¾ä¸­ï¼Œåˆ›é€ äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ã€‚
<div align="center">
  <img width="674" height="142" alt="image" src="https://github.com/user-attachments/assets/cf3a2bc5-8fe1-477f-a18f-dace70a3dc2a" />
</div>

ç”±æ­¤è¡¨å¯è§åœ¨è†¨èƒ€ç‡20ï¼Œæ”¯æŒæ©ç å…·æœ‰ä¸¥é‡å™ªå£°æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä»ç„¶ä¿æŒä¸€å®šçš„æ€§èƒ½ï¼Œè¯æ˜äº†æˆ‘ä»¬æ¨¡å‹é¢å¯¹å«å™ªå£°æ”¯æŒæ©ç çš„å®éªŒé²æ£’æ€§

### CTSGM and Generalization of the model
We followed the standard protocol by using the template "a photo of a {class}" for text embeddings. Additionally, we explicitly verified the generalization effectiveness of our method through cross-domain experiments transferred from COCO-20i to PASCAL-5i.
æœ¬æ–‡å®éªŒé‡‡ç”¨çš„æ˜¯æ ‡å‡†æ¨¡æ¿ï¼ša photo of a {class},è¿›ä¸€æ­¥æœ¬æ–‡åœ¨è·¨æ•°æ®é›†ä¸Šå®éªŒCOCO-20i to PAscal-5iéªŒè¯å…¶æ³›åŒ–æœ‰æ•ˆæ€§.

<div align="center">
  <img width="615" height="190" alt="image" src="https://github.com/user-attachments/assets/718d1ec8-e904-4da9-9289-de64d345c1fd" />
</div>

### Response to Reviewer#2
# Experimental fairness
To address fairness concerns arising from differing resolution settings, we conducted balanced comparative experiments at the two commonly used resolutions in this field: 473Ã—473 and 448Ã—448. The experimental results demonstrate that FAMANet exhibits strong robustness to input size variations, with performance fluctuations across different resolutions remaining within 0.3%. More importantly, even after strictly aligning the resolution settings, our method maintains superior performance. This fully confirms that the performance improvement is primarily attributable to enhancements in network architecture rather than differences in experimental configuration. All subsequent ablation studies were conducted on the Pascal-5i dataset.
<div align="center">
  <img width="672" height="115" alt="image" src="https://github.com/user-attachments/assets/c03113d5-4521-4f0c-91ea-e38be0759450" />
</div>

### Implementation Details
To ensure result reproducibility, our experimental setup strictly follows the conditions of HSNet and DCAMA, with specific adjustments: we employ the Adam optimizer (initial learning rate set to 1e-4) and train for 100 epochs until convergence. Additionally, consistent with HSNet's configuration, no extra data augmentation strategies are introduced during this training phase. All experiments are conducted on four NVIDIA RTX 4090 GPUs.

-**Source Code**: The implementation of train can be found in [`train.sh`](./scripts/train.sh). 
- **Source Code**: The implementation of test can be found in [`test.sh`](./scripts/test.sh). 
- **Source Code**: The implementation of config can be found in [`config.py`](./common/config.py). 


# Frequency-enhanced Affinity Map Weighted Mask Aggregation for Few-Shot Semantic Segmentation

<div align="center">

<!-- You can add badges here if you have them, e.g., PyTorch version, License -->
<!-- ![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white) -->

</div>

## ğŸ“– Introduction

This repository contains the implementation of **Frequency-enhanced Affinity Map Weighted Mask Aggregation (FAMANet)** for Few-Shot Semantic Segmentation.

## ğŸ—ï¸ Network Architecture

The overall architecture of our proposed method is shown below:

<div align="center">
  <img width="1489" height="728" alt="image" src="https://github.com/user-attachments/assets/0b97624d-6686-45bc-ae6b-e5a15daa0c33" />
</div>

---

## ğŸ§© Key Modules

### 1. Phase and Amplitude Attention Module (PAAM)

PAAM is designed to enhance feature representation by utilizing frequency domain information.

- **Source Code**: The implementation of PAAM can be found in [`PhaseandAmplitudeAttention.py`](./model/mymodule/PhaseandAmplitudeAttention.py). 

<div align="center">
  <img width="1525" height="472" alt="image" src="https://github.com/user-attachments/assets/b457744b-62f2-47bc-aea4-75f6335c04b2" />
</div>

#### Visualization of PAAM Effects
Visual comparison showing the effectiveness of the frequency-enhanced attention:

<div align="center">
  <img width="1000" height="705" alt="image" src="https://github.com/user-attachments/assets/c2b25ca6-6ef1-4290-86a7-5ecfeb1bd913" />
</div>

### 2. Affinity Map Aggregation Module (AMAM)

AMAM utilizes cross-attention mechanisms to aggregate mask weights based on affinity maps.

- **Source Code**: The implementation of AMAM can be found in [`CrossAttention.py`](./model/mymodule/CrossAttention.py). 

<div align="center">
  <img width="973" height="298" alt="image" src="https://github.com/user-attachments/assets/8e7f6930-a4b8-4616-8a27-2e431c096621" />
</div>

---
###   DataSet
<div align="center">
  <img width="729" height="753" alt="image" src="https://github.com/user-attachments/assets/55194873-f3ce-4b0e-83cf-45745eeb3098" />

</div>
<div align="center">
  <img width="729" height="753" alt="image" src="https://github.com/user-attachments/assets/aa96ecb9-886a-4430-860d-d4ce653b1ed5" />
</div>



## ğŸš€ Getting Started

### Training
To train the model, please verify the configurations in the script and run:

```bash
bash train.sh

### Testing
To test the model, please verify the configurations in the script and run:

```bash
bash test.sh

